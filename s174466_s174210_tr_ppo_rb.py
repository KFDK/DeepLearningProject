"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8z3Na11il4rl_XJrSO6FYptinBJFpWl
Original file created by Nicklas Hansen
    

Nicolai Weisbjerg & Kelvin Foster (s174466, s174210)
28-12-2020

"""


import numpy as np


################### Configuration values of run ####################
# User defined name for run
setname = 'TestRun'

# 1 for running TrulyPPO, 0 for PPO
trpporb = 0;

#Include evaluation of agent pr timestep (1 for yes)
evaluation = 0;         

# Hyperparameters
env_name = 'chaser'             # Name of game enviroment
total_steps = 2e7               # Number of timesteps taken during training
num_envs = 32                   # Number of parallel training enviroments during sampling
num_levels = 100                # Number of distinct levels that the agent is exposed to during training. Higher number -> better generalization
num_steps = 256                 # Number of steps taken during generation of samples (pr. num_envs)
num_epochs = 3                  # Number of times the generated data (training data) is cycled during policy optimization
batch_size = np.int(num_envs*num_steps/4)      # Batch size during policy optimization
grad_eps = .5                   # Cut-off value for graident clipping
seed = 4;                       # Seed of run

# PPO and TrulyPPO specific hyperparameters
eps = 0.2                       # Clipping function level (both for algoritm specfic objective function and squared-error loss of value function)
value_coef = .5                 # Weighing value function in overall objective function
entropy_coef = 0.01             # Weighing entropy bonus in overall objective function
feature_dim = 512               # CNN-network output dim
           
# TrulyPPO specific hyperparameters
alpha = 1
delta = 0.5

# Adjust delta value along the training duration
linanneal = 0;
linannealdrop_rate = 0.97 #delta = delta * linannealdrop_rate every 1 percent point







################### Import packages and librarys ##################

import time
import os
import torch
import torch.nn as nn
from utils_savior import make_env, Storage, orthogonal_init
#!pip install procgen









########## Write hyperparameters to run specific file ###############

if trpporb == 1:
    algo = 'TrulyPPO'
else:
    algo = 'PPO'
    
if linanneal == 1:
    lina = 'yes'
else:
    lina = 'no'   

if evaluation == 1:
    evaluationstr = 'yes'
else:
    evaluationstr = 'no'
    
#Timestamp in front of test-name
timestr = time.strftime("%Y-%m-%d_%Hh_%Mm_%Ss");
setname = timestr + '_' + setname + '/' + timestr + '_' + setname;
#Make folder
os.makedirs(os.path.dirname(setname))


#Write Hyperparameters to file
filename = setname + '_hyper.txt'
file1 = open(filename,"w")

file1.writelines(['total_steps: ' + str(total_steps) + '\n',
                  'num_envs: ' + str(num_envs) + '\n',
                  'num_levels: ' + str(num_levels) + '\n',
                  'num_steps: ' + str(num_steps) + '\n',
                  'num_epochs: ' + str(num_epochs) + '\n',
                  'batch_size: ' + str(batch_size) + '\n',
                  'eps: ' + str(eps) + '\n',
                  'grad_eps: ' + str(grad_eps) + '\n',
                  'value_coef: ' + str(value_coef) + '\n',
                  'entropy_coef: ' + str(entropy_coef) + '\n',
                  'alpha: ' + str(alpha) + '\n',
                  'delta: ' + str(delta) + '\n',
                  'algorithm: ' + algo + '\n',
                  'linanneal: ' + lina + '\n',
                  'linannealdrop_rate: ' + str(linannealdrop_rate) + '\n',
                  'feature_dim: ' + str(feature_dim) + '\n',
                  'env_name: ' + env_name +'\n',
                  'seed: ' + str(seed) +'\n',
                  'evaluation: ' + evaluationstr])

file1.close()

filenameout = setname + '_output.txt'
fileout = open(filenameout,"w")










################# Network definition (policy of agent) ##################

#Simple flatten class
class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

# Nature CNN encoder
class Encoder(nn.Module):
  def __init__(self, in_channels, feature_dim):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),
        Flatten(),
        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()
    )
    self.apply(orthogonal_init)

  def forward(self, x):
    return self.layers(x)



# Policy definition
class Policy(nn.Module):
  def __init__(self, encoder, feature_dim, num_actions):
    super().__init__()
    self.encoder = encoder
    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)
    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)

  def act(self, x):
    with torch.no_grad():
      x = x.cuda().contiguous()
      dist, value, logits = self.forward(x)
      action = dist.sample()
      log_prob = dist.log_prob(action)

    return action.cpu(), log_prob.cpu(), value.cpu(), logits.cpu()

  def forward(self, x):
    x = self.encoder(x)
    logits = self.policy(x)
    value = self.value(x).squeeze(1)
    dist = torch.distributions.Categorical(logits=logits)

    return dist, value, dist.logits









#################### Define training environment ####################

# check the utils.py file for info on arguments
env = make_env(env_name=env_name,n_envs=num_envs, num_levels=num_levels,seed=seed)
fileout.write('Observation space: ' + str(env.observation_space) + '\n')
fileout.write('Action space: ' + str(env.action_space.n) + '\n')
fileout.close()
fileout = open(filenameout,"a")

# Define network
encoder = Encoder(env.observation_space.shape[0], feature_dim)
policy = Policy(encoder, feature_dim, env.action_space.n)
policy.cuda()

# Define optimizer
# these are reasonable values but probably not optimal
optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)

# Define temporary storage
# we use this to collect transitions during each iteration
storage = Storage(
    env.observation_space.shape,
    num_steps,
    num_envs,
    env.action_space.n
)
fileout.write(str(storage.advantage.shape) + '\n\n\n')
fileout.close()
fileout = open(filenameout,"a")








#################### Define evaluation environment ##########################
# Only define evaluation enviroment if wanted by the user
if evaluation == 1:
    #Make evaluation enviroment
    eval_env = make_env(env_name=env_name, n_envs=num_envs, start_level=2*num_levels, num_levels=num_levels)
    obs_eval = eval_env.reset()
    
    # Define evaluation temporary storage
    # we use this to collect transitions during each iteration
    storage_eval = Storage(
        env.observation_space.shape,
        num_steps,
        num_envs,
        env.action_space.n
    )
    
    DictEval = {'Score_eval' : []}
    
    
    
    
    
    



######################## Run training ############################
# Start time to measure time of training
starttime = time.time();


fileout.write('Training starts\n')
obs = env.reset()
step = 0

# Create dict for saving results (metrics of training)
DictRes = {'Step':[], 'Reward':[], 'Entropy':[], 'KLdiv': []}


while step < total_steps:

  ########### Generate samples ############
  
  # Use policy to collect data for num_steps steps
  policy.eval()
  for _ in range(num_steps):
    # Use policy
    action, log_prob, value, logits = policy.act(obs)

    # Take step in environment
    next_obs, reward, done, info = env.step(action)

    # Store data
    storage.store(obs, action, reward, done, info, log_prob, value, logits)

    # Update current observation
    obs = next_obs

  # Add the last observation to collected data
  _, _, value, _ = policy.act(obs)
  storage.store_last(obs, value)

  # Compute return and advantage
  storage.compute_return_advantage()

  



  ############# Optimize policy ###############
  
  policy.train()
  for epoch in range(num_epochs):
    
    # Iterate over batches of transitions
    generator = storage.get_generator(batch_size)
    for batch in generator:
      #pdb.set_trace()
      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage, old_logits = batch

      # Get current policy outputs
      new_dist, new_value, _ = policy(b_obs)
      old_dist = torch.distributions.Categorical(logits=old_logits)

      # Get new log probs
      new_log_prob = new_dist.log_prob(b_action)
      # Clipped policy objective
      ratio = torch.exp(new_log_prob-b_log_prob)
      
      if trpporb == 1:
        KLdiv = torch.distributions.kl.kl_divergence(old_dist,new_dist)
        pi_loss = ratio*b_advantage - alpha*KLdiv*(KLdiv >= delta)*(ratio*b_advantage >= b_advantage)
        pi_loss = -torch.mean(pi_loss)
      else:
        KLdiv = torch.distributions.kl.kl_divergence(old_dist,new_dist)
        clipped_ratio = ratio.clamp(min=1.0-eps, max=1.0+eps)
        pi_loss = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)
        pi_loss = -torch.mean(pi_loss)

      KLdivmax = torch.max(KLdiv[torch.isinf(KLdiv)==False])

      # Clipped value function objective
      clipped_value = b_value + (new_value - b_value).clamp(min=-eps,max=eps)
      value_loss = torch.max((new_value - b_returns)**2, (clipped_value - b_returns)**2)
      value_loss = 0.5 * torch.mean(value_loss)

      # Entropy loss
      #entropy_loss = -torch.sum(torch.exp(new_log_prob)*new_log_prob)
      entropy_loss = new_dist.entropy().mean() # from Nicklas code

      # Backpropagate losses
      loss = pi_loss + value_loss*value_coef - entropy_loss*entropy_coef
      loss.backward()

      # Clip gradients
      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)
      
      # Update policy
      optimizer.step()
      optimizer.zero_grad()
      
      # Save imediate results
      DictRes['Step'].append(step)
      DictRes['Reward'].append(storage.get_reward().item())
      DictRes['Entropy'].append(new_dist.entropy().mean().item())
      DictRes['KLdiv'].append(KLdivmax.item())

  # Update stats
  step += num_envs * num_steps
  
  
  
  
  
  

  ######## Write progress to output file (and update delta) ##########
  if (np.floor(step/total_steps*100) != np.floor((step-num_envs * num_steps)/total_steps*100)):
    timenow = time.time();
    timediffh = str(np.int(np.floor((timenow - starttime)/3600)))
    timediffm = str(np.int(np.floor((timenow - starttime)/60-np.floor((timenow - starttime)/3600)*60)))
    fileout.write('Prog: ' + str(np.floor(step/total_steps*100)) +
          '%\tMean reward: ' + str(storage.get_reward()) +
          '\t Time Elapsed: ' + timediffh + 'h ' + timediffm + 'm\n')
    fileout.close()
    fileout = open(filenameout,"a")
    
    # If linanneal on delta value should be altered during training
    if (linanneal == 1 and trpporb == 1):
      delta = delta*linannealdrop_rate
      
      
      
      
      
  ######### Evaluate agent on evaluation enviroment ###########
  if evaluation == 1:
      policy.eval()
      for _ in range(num_steps):
        # Use policy
        action, log_prob, value, logits = policy.act(obs_eval)
    
        # Take step in environment
        next_obs, reward, done, info = eval_env.step(action)
    
        # Store data
        storage_eval.store(obs_eval, action, reward, done, info, log_prob, value, logits)
    
        # Update current observation
        obs_eval = next_obs
    
      # Add the last observation to collected data
      _, _, value, _ = policy.act(obs_eval)
      storage_eval.store_last(obs_eval, value)
    
      # Compute return and advantage
      storage_eval.compute_return_advantage() 
      
      DictEval['Score_eval'].append(storage_eval.get_reward().item())

  












############## Save results and write to files ############

fileout.write('Completed training!' + '\n\n')
fileout.close()
fileout = open(filenameout,"a")
torch.save(policy.state_dict(), setname + '_checkpoint.pt')


#Dump results to pickle file
import pickle
resname = setname + '_results.pkl'
f = open(resname,"wb")
pickle.dump(DictRes,f)
f.close()


#Dump evaluation results to file
if evaluation == 1:
  evalname = setname + '_eval.pkl'
  f = open(evalname,"wb")
  pickle.dump(DictEval,f)
  f.close()










##### Make final agent evaluation and create video of agent playing the game ########
import imageio

#Frames list for creation of video
frames = []



######### Make storage for evaluation environment ###########
if evaluation != 1:
    # Define evaluation temporary storage
    # we use this to collect transitions during each iteration
    storage_eval = Storage(
        env.observation_space.shape,
        num_steps,
        num_envs,
        env.action_space.n
    )



######## Evaluate policy on evaluation enviroment ########
liste = []
for i in range(10): 
  # check the utils.py file for info on arguments
  env = make_env(env_name=env_name,n_envs=num_envs, num_levels=num_levels,start_level=2*num_levels,seed=i) 
  obs = env.reset()
  
  for _ in range(num_steps):

    # Use policy
    action, log_prob, value, logits = policy.act(obs_eval)
  
    # Take step in environment
    next_obs, reward, done, info = eval_env.step(action)
  
    # Store data
    storage_eval.store(obs_eval, action, reward, done, info, log_prob, value, logits)
  
    obs_eval = next_obs

    # Render environment and store
    if i == 0:
      frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()
      frames.append(frame)


  #Append average return to list
  liste.append(storage_eval.get_reward().item())
  
# Calculate average non-discounted, non-normalized return
liste = np.array(liste)
fileout.write('Average reward: ' + str(np.mean(liste)) + '\n')




############## Save frames as video ###################
frames = torch.stack(frames)
imageio.mimsave(setname + '_vid.mp4', frames, fps=25)
fileout.close()
